<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="日拱一卒，功不唐捐"><title>Marcovaldo</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Marcovaldo</h1><a id="logo" href="/.">Marcovaldo</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h2 class="post-title"><a href="/2016/05/18/机器学习中使用的神经网络第七讲/">机器学习中使用的神经网络第七讲</a></h2><div class="post-meta">2016-05-18</div><a data-thread-key="2016/05/18/机器学习中使用的神经网络第七讲/" href="/2016/05/18/机器学习中使用的神经网络第七讲/#comments" class="ds-thread-count"></a><div class="post-content"><p>Geoffery Hinton教授的<em>Neuron Networks for Machine Learning</em>的第七讲介绍了循环神经网络（recurrent neural network, RNN）和Long Short Term Memory。</p>
<h2 id="Modeling-sequences-A-brief-overview"><a href="#Modeling-sequences-A-brief-overview" class="headerlink" title="Modeling sequences: A brief overview"></a>Modeling sequences: A brief overview</h2><p>在这一小节，我们将对应用于序列（sequences）的不同类型的模型做一个概括。我们从最简单的模型——ultra aggressive models（该模型尝试根据前一个序列（term or sequence）去预测下一个序列）开始，接着再讨论该模型使用了隐含层的复杂变形，然后再介绍更多有着hidden state和hidden dynamics的模型（这其中包含了linear dynamics systems和hidden Markov models）。这些模型都与RNN有关，因此先介绍，有兴趣了解模型细节的请自行搜索。</p>
<p>当我们用机器学习来构建序列时，我们通常想要将一种序列转换到另一种序列。例如，我们希望能将英语转换成法语，语音识别中我们希望将语音序列转换成词汇序列等。有时，目标序列不是孤立的，我们可以得到一种教学信号来尝试从输入序列中预测下一个序列，因此目标输出序列只是有着进一步形式的输入序列。比起根据一张图像中所有其他像素点去预测一个像素点或者根据一张图像的其他部分来预测一个部分，这种序列的转换看起来更自然。原因是，对于时间序列来讲会有一个很自然的顺序来预测下一步，而对图像处理来讲你不知道接下来的预测是基于什么（不过类似的方法很适用于图像处理）。另外，在进行序列转换时，监督式学习与非监督式学习的区别可能就模糊了（不太明白，但感觉不是很重要）。<br></div><p class="readmore"><a href="/2016/05/18/机器学习中使用的神经网络第七讲/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/05/16/机器学习中使用的神经网络第六讲/">机器学习中使用的神经网络第六讲</a></h2><div class="post-meta">2016-05-16</div><a data-thread-key="2016/05/16/机器学习中使用的神经网络第六讲/" href="/2016/05/16/机器学习中使用的神经网络第六讲/#comments" class="ds-thread-count"></a><div class="post-content"><p>Geoffery Hinton教授的<em>Neuron Networks for Machine Learning</em>的第六讲介绍了随机梯度下降法（SGD），并且介绍了加快学习速度的动量方法（the momentum method）、针对网络中每一个连接的自适应学习步长（adaptive learning rates for each connection）和RMSProp算法。</p>
<p>这几个算法的难度很大，需要反复推理思考，并在实践中摸索以加深理解。</p>
<h2 id="Overview-of-mini-batch-gradient-descent"><a href="#Overview-of-mini-batch-gradient-descent" class="headerlink" title="Overview of mini-batch gradient descent"></a>Overview of mini-batch gradient descent</h2><p>这一小节介绍随机梯度下降法（stochastic gradient descent）在神经网络中的使用，随机梯度下降法在Andrew Ng的课程中已有介绍，所以这里可能会有所简略，大家可以阅读<a href="http://blog.csdn.net/majordong100/article/details/51150623">Machine Learning第十周笔记：大规模机器学习</a></p>
<p>这里首先回顾了<a href="http://blog.csdn.net/majordong100/article/details/51321654">第三讲</a>中介绍的线性神经网络的误差曲面(error surface)，如下图所示。线性神经网络对应的误差曲面的纵截面如碗装，横截面则如一组同心的椭圆。梯度下降法的作用就是不断调整参数，使得模型的误差由“碗沿”降到“碗底”，参数由椭圆外部移动到椭圆的中心附近。当然，这里说的是只有两个参数的情况，参数更多的情况则更复杂。<br></div><p class="readmore"><a href="/2016/05/16/机器学习中使用的神经网络第六讲/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/05/14/机器学习中使用的神经网络第五讲：CNN/">机器学习中使用的神经网络第五讲：CNN</a></h2><div class="post-meta">2016-05-14</div><a data-thread-key="2016/05/14/机器学习中使用的神经网络第五讲：CNN/" href="/2016/05/14/机器学习中使用的神经网络第五讲：CNN/#comments" class="ds-thread-count"></a><div class="post-content"><p>Geoffery Hinton教授的<em>Neuron Networks for Machine Learning</em>的第五讲主要介绍物体识别问题的难点及克服这些难点的方法，重点介绍了数字识别和物体识别中使用的卷积网络。</p>
<h2 id="Why-object-recognition-is-difficult"><a href="#Why-object-recognition-is-difficult" class="headerlink" title="Why object recognition is difficult"></a>Why object recognition is difficult</h2><p>我们知道识别真实场景中的物体是很困难的，这一小节我们来介绍造成这些困难的一些东西。</p>
<ul>
<li>Segmentation: 在一个图像中，我们很难将其中的一个物体与其他的物体分隔开。在现实生活中，我们人类有两只眼睛且我们身体可以移动，这样在视觉上可以很容易做到分辨物体。而图像是静态的，且一个物体很可能被另一个物体遮住一部分，这就造成了信息的缺失。</li>
<li>Lighting: 像素点的密度/亮度（intensity）是由物体的亮度决定的，我们从不同亮度的图像中得到的信息是不同的。</li>
<li>Deformation: 非仿射方式的变形也使得识别变得困难</li>
<li>Affordances: 这里还涉及到功能可见性（affordance）的问题，有很多物体是从用途角度去定义的，而非从视觉角度，例如椅子有着各种各样的物理性状。</li>
<li>Viewpoint: 视角的变化会造成图像的变化，而标准的学习方法是无法应付的。</li>
</ul></div><p class="readmore"><a href="/2016/05/14/机器学习中使用的神经网络第五讲：CNN/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/05/08/机器学习中使用的神经网络第四讲/">机器学习中使用的神经网络第四讲</a></h2><div class="post-meta">2016-05-08</div><a data-thread-key="2016/05/08/机器学习中使用的神经网络第四讲/" href="/2016/05/08/机器学习中使用的神经网络第四讲/#comments" class="ds-thread-count"></a><div class="post-content"><p>Geoffery Hinton教授的<em>Neuron Networks for Machine Learning</em>的第四讲主要介绍如何使用back propagation算法来学习到词汇的特征表示、Neuron-probabilistic language models和处理大规模输出的方法。</p>
<h2 id="Learning-to-predict-the-next-word"><a href="#Learning-to-predict-the-next-word" class="headerlink" title="Learning to predict the next word"></a>Learning to predict the next word</h2><p>接下来的几小节主要介绍如何使用back propagation算法来学习到词汇的特征表示。我们从一个很简单的例子开始，介绍使用back propagation算法来将词汇间的相关信息转换成特征向量。</p>
<p>下图给出了一个家庭的树状图，我们要做的就是让神经网络去理解树状图中的信息，将其中的信息翻译成一个个命题，如下面第二张图所示。<br></div><p class="readmore"><a href="/2016/05/08/机器学习中使用的神经网络第四讲/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/05/05/机器学习中使用的神经网络第三讲：线性-逻辑神经网络和BackPropagation/">机器学习中使用的神经网络第三讲：线性/逻辑神经网络和BackPropagation</a></h2><div class="post-meta">2016-05-05</div><a data-thread-key="2016/05/05/机器学习中使用的神经网络第三讲：线性-逻辑神经网络和BackPropagation/" href="/2016/05/05/机器学习中使用的神经网络第三讲：线性-逻辑神经网络和BackPropagation/#comments" class="ds-thread-count"></a><div class="post-content"><p>Geoffrey Hinton教授的<em>Neuron Networks for Machine Learning</em>的第三讲主要介绍了线性/逻辑神经网络和BackPropagation，下面是整理的笔记。</p>
<h2 id="Learning-the-weights-of-a-linear-neuron"><a href="#Learning-the-weights-of-a-linear-neuron" class="headerlink" title="Learning the weights of a linear neuron"></a>Learning the weights of a linear neuron</h2><p>这一小节介绍线性神经网络的学习算法。线性神经网络很像感知机，但又有不同：在感知机中，权值向量总是越来越接近好的权值设定；在线性神经网络中，输出总是越来越接近目标输出。在感知机中，每一次更新权值向量，其就更接近每一个“一般可行”的权值向量，这限制了感知机不能应用于更加复杂的网络，因为两个好的权值向量的平均可能是一个坏的。故在多层神经网络中，我们不能使用感知机的学习流程，也不能使用类似的方法来证明学习的可行性。</p>
<p>在多层神经网络中，我们通过判断实际输出是否越来越接近目标输出来判断学习的性能是否在提高。这一策略在解决非凸问题时仍然奏效，但不适合应用于感知机的学习。最简单的例子是使用平方误差的线性神经网络（linear neurons），也称为线性过滤器（linear filter）。如下图所示，y是神经网络对期望输出的一个估计，w是权值向量，x是输入向量，学习的目标是最小化在所有训练样本上犯的错误之和。<br></div><p class="readmore"><a href="/2016/05/05/机器学习中使用的神经网络第三讲：线性-逻辑神经网络和BackPropagation/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/05/04/机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机/">机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机</a></h2><div class="post-meta">2016-05-04</div><a data-thread-key="2016/05/04/机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机/" href="/2016/05/04/机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机/#comments" class="ds-thread-count"></a><div class="post-content"><p>最近在Cousera上学习多伦多大学Geoffrey Hinton教授的<em>Nerual Networks for Machine Learning</em>，为保证学习效果，特整理了学习笔记，一方面加深理解，一方面试图将学到的东西讲清楚。</p>
<p>这一讲主要介绍神经网络的结构。</p>
<h2 id="Types-of-nerual-network-architectures"><a href="#Types-of-nerual-network-architectures" class="headerlink" title="Types of nerual network architectures"></a>Types of nerual network architectures</h2><p>这一小节介绍了三种不同的神经网络结构。</p>
<p>首先介绍向前反馈网络（feed forward network），其常见形式如下图所示，第一层是输入（input layer），最后一层是输出（output layer），中间是一层或多层隐匿单元（hidden layer，被称之为deep nerual network）。<br></div><p class="readmore"><a href="/2016/05/04/机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/04/26/机器学习基石第九讲：linear-regression/">机器学习基石第九讲：linear regression</a></h2><div class="post-meta">2016-04-26</div><a data-thread-key="2016/04/26/机器学习基石第九讲：linear-regression/" href="/2016/04/26/机器学习基石第九讲：linear-regression/#comments" class="ds-thread-count"></a><div class="post-content"><p><em>机器学习基石</em>第十讲介绍线性回归问题（linear regression problem），从这一讲开始课程介绍具体的机器学习算法。后面的大部分内容，博主已经学过，所以笔记可能会简略。</p>
<h2 id="Linear-Regression-Problem"><a href="#Linear-Regression-Problem" class="headerlink" title="Linear Regression Problem"></a>Linear Regression Problem</h2><p>借助信用卡发放的问题来介绍线性回归，不过这一次不再是分类，而是要让算法根据客户信息给出信用额度。算法为每一个特征赋予一个权重，然后直接计算加权值，以此得到信用额度。<br></div><p class="readmore"><a href="/2016/04/26/机器学习基石第九讲：linear-regression/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/04/25/机器学习基石第八讲：noise-and-error/">机器学习基石第八讲：noise and error</a></h2><div class="post-meta">2016-04-25</div><a data-thread-key="2016/04/25/机器学习基石第八讲：noise-and-error/" href="/2016/04/25/机器学习基石第八讲：noise-and-error/#comments" class="ds-thread-count"></a><div class="post-content"><p><em>机器学习基石</em>第八讲主要介绍噪声和误差度量，笔记整理在下面。</p>
<h2 id="Noise-and-Probabilistic-Target"><a href="#Noise-and-Probabilistic-Target" class="headerlink" title="Noise and Probabilistic Target"></a>Noise and Probabilistic Target</h2><p>现实中的数据很可能含有噪声（noise），例如前面的信用卡发放问题中，有的顾客符合发放标准但没有发给，或者同样情况的顾客有人发了有人没法，再或者顾客的信息不正确等等，VC bound是否在由噪声的情况下工作呢？</p>
<p>继续使用前面抽弹珠的例子，罐子中每一个弹珠代表一个数据，如果$f(x) \not= h(x)$，则将弹珠漆成橘色；如果$f(x) = h(x)$，则将弹珠漆成绿色。引入了噪声之后，原来的y就不是真正的f(x)了。现在想象这样每一个弹珠，它的颜色不停变化，称为probablistic(noisy) marbles，不过罐子内橘色弹珠的比例还是一定的。现在我们记录弹珠被抽出瞬间的颜色，比如一次抽取100颗弹珠，抽出瞬间有20颗是橘色的，那我们就可以估计抽出瞬间整个罐子的橘色弹珠比例为20%。<br></div><p class="readmore"><a href="/2016/04/25/机器学习基石第八讲：noise-and-error/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/04/25/机器学习基石第七讲：the-vc-dimension/">机器学习基石第七讲：the vc dimension</a></h2><div class="post-meta">2016-04-25</div><a data-thread-key="2016/04/25/机器学习基石第七讲：the-vc-dimension/" href="/2016/04/25/机器学习基石第七讲：the-vc-dimension/#comments" class="ds-thread-count"></a><div class="post-content"><p><em>机器学习基石</em>第七讲主要介绍了VC dimension，笔记整理在下面。</p>
<h2 id="Definition-of-VC-Dimension"><a href="#Definition-of-VC-Dimension" class="headerlink" title="Definition of VC Dimension"></a>Definition of VC Dimension</h2><p>上一讲我们找到了B(N,k)的上限，拿它和$N^{k-1}$做一个比较，发现当N够大时，前者比后者小得多。<br></div><p class="readmore"><a href="/2016/04/25/机器学习基石第七讲：the-vc-dimension/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/04/24/机器学习基石第六讲：theory-of-generalization/">机器学习基石第六讲：theory of generalization</a></h2><div class="post-meta">2016-04-24</div><a data-thread-key="2016/04/24/机器学习基石第六讲：theory-of-generalization/" href="/2016/04/24/机器学习基石第六讲：theory-of-generalization/#comments" class="ds-thread-count"></a><div class="post-content"><p><em>机器学习基石</em>第六讲继续讨论“学习是否可行的问题”。</p>
<h2 id="Restriction-of-Break-Point"><a href="#Restriction-of-Break-Point" class="headerlink" title="Restriction of Break Point"></a>Restriction of Break Point</h2><p>继续前面的讨论，我们看$m_H(N)$是否会有一个很小的增长速度。回顾前面的四种成长函数及其break point。我们知道k是一个成长函数的break point，那比k大的值全是break point。<br></div><p class="readmore"><a href="/2016/04/24/机器学习基石第六讲：theory-of-generalization/">阅读更多</a></p></div><nav class="page-navigator"><a class="extend prev" rel="prev" href="/">上一页</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/">下一页</a></nav></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://marcovaldong.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/Neural-Network/">Neural Network</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书/">读书</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/读书/" style="font-size: 15px;">读书</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Theano/" style="font-size: 15px;">Theano</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/Kaggle/" style="font-size: 15px;">Kaggle</a> <a href="/tags/Neural-Network/" style="font-size: 15px;">Neural Network</a> <a href="/tags/机器学习基石/" style="font-size: 15px;">机器学习基石</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/06/27/MLDS2017Final/">MLDS2017Final</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/05/14/使用Tensorflow实现Titanic比赛/">使用Tensorflow实现Titanic比赛</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/19/Python爬虫小结之Selenium/">Python爬虫小结之Selenium</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/11/My-reading-list/">My reading list</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/08/18/Python爬虫爬取知乎小结/">Python爬虫爬取知乎小结</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/07/20/Theano实现kaggle手写识别/">Theano实现kaggle手写识别</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/07/08/Ubuntu14-04-Theano-OpenCL-libgpuarray实现GPU运算/">Ubuntu14.04+Theano+OpenCL+libgpuarray实现GPU运算</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/06/12/使用Theano实现kaggle手写识别/">使用Theano实现kaggle手写识别</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/30/机器学习中使用的神经网络第十讲/">机器学习中使用的神经网络第十讲</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/21/机器学习中使用的神经网络第九讲/">机器学习中使用的神经网络第九讲</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Marcovaldo.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'marcovaldo'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?2be92f134440f46356c71aa55035a144";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>